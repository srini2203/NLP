# -*- coding: utf-8 -*-
"""Hindle and Rooth.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nnp4tQF21pLvMHcv_Llyb53F1vLsFBI6
"""

import nltk
import math
import re
from nltk.tokenize import word_tokenize
from nltk import bigrams

# Download necessary resources
nltk.download('punkt')

# Read the corpus from the file
with open('corpus.txt', 'r', encoding='utf-8') as file:
    corpus = file.read()

# Tokenize the corpus into words using regex (to avoid punctuation issues)
tokens = re.findall(r'\b\w+\b', corpus.lower())

# Count occurrences of individual words
element_counts = {}
for element in tokens:
    element_counts[element] = element_counts.get(element, 0) + 1

# Generate bigrams from tokenized words
bi_grams = list(bigrams(tokens))

# Count the frequency of each bigram
bigram_counts = {}
for bigram in bi_grams:
    bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1

# User input for noun, verb, and preposition
noun = input("Enter the Noun: ").lower()
verb = input("Enter the Verb: ").lower()
prep = input("Enter the Preposition: ").lower()

# Retrieve counts
n = element_counts.get(noun, 0)
v = element_counts.get(verb, 0)
p_n = bigram_counts.get((prep, noun), 0)
p_v = bigram_counts.get((prep, verb), 0)

# Function to calculate probabilities
def cal_prob(p_v, p_n, v, n):
    prob_v = (p_v / v) if v > 0 else 0
    prob_n = (p_n / n) if n > 0 else 0
    return prob_v, prob_n

prob_v, prob_n = cal_prob(p_v, p_n, v, n)

# Function to calculate lambda (handling zero cases)
def cal_lam(prob_v, prob_n):
    if prob_n == 0:  # Avoid division by zero
        return float('-inf')
    _lambda = math.log((prob_v * (1 - prob_n)) / prob_n, 2)
    return _lambda

_lambda = cal_lam(prob_v, prob_n)

# Determine preposition attachment
if _lambda > 0:
    print("The Preposition is attached with Verb.")
elif _lambda < 0:
    print("The Preposition is attached with Noun.")
else:
    print("The Preposition attachment cannot be determined.")